做出的修改如下：
1.source里面的ORTOOLS文件是外部的传统求解器，这里用的是谷歌的开源求解器框架or-tools，求解的时候需要先初始化一个TSPSolver求解器，然后用其.solve方法输入一个2d的numpy序列，第一个维度为节点数量，第二个维度为节点属性个数，然后最后返回一个最优解序列。（slover-test是调用千问3max做的对LLM的实验）
2.修改了travelling_saleman_problem里面dataloader等逻辑，现在dataloader中是一个元组，元组的第一个元素是节点结构和图结构，维度是（batch，nodesize，2），第二个元素是最优解的route，维度是（batch，nodesize+1）这个+1是因为对TSP问题来说路径是回环，加上起点就是+1
3.修改了Train_Grouped_Actors里面训练的逻辑，一次循环做两次训练，首先把状态先推到最优解route一半的状态，然后更新firstmove，之后一样的逻辑去进行采样。
4.需要注意的是，dataset每次都是更新的，也就是说，我们实际上是用很多实例，让模型学习到进入最优部分状态之后如何获得很好的奖励，从而大大降低其采样到好的奖励的难度，而不是单个单个的让模型一对一的记住进入这种状态之后怎么做才能好，也符合原文的逻辑。
